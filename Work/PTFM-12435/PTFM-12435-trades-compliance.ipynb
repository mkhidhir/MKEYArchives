{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import csv\n",
    "import time\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "from pandas import json_normalize\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Key and Default Parameters\n",
    "# User-specific API key\n",
    "API_KEY = 'API_KEY'  # Replace with your actual API key\n",
    "API_KEY_MT = 'API_KEY_MT' #MT Vessel Positions API Key\n",
    "\n",
    "# Default API parameters\n",
    "DEFAULT_ORIGIN_DATE_START = '2024-10-15'       # Set a default origin start date\n",
    "DEFAULT_ORIGIN_DATE_END = '2024-10-24'         # Set a default origin end date\n",
    "DEFAULT_DESTINATION_DATE_START = None # Optional, leave None if not required\n",
    "DEFAULT_DESTINATION_DATE_END = None            # Optional, leave None if not required\n",
    "DEFAULT_VESSELS = []                           # Optional, list of vessels as strings\n",
    "DEFAULT_TRADE_STATUS = 'in transit'            # By Trade Status\n",
    "DEFAULT_SIZE = 100                             # Set Data Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_api_params():\n",
    "    params = {\n",
    "        'tradeStatus': input(f\"Enter Trade Status (default: {DEFAULT_TRADE_STATUS }): \") or DEFAULT_TRADE_STATUS \n",
    "        'size': input(f\"Enter Data Limit (default: {DEFAULT_SIZE}): \") or DEFAULT_SIZE\n",
    "    }\n",
    "    return params\n",
    "\n",
    "# Construct Trades API URL\n",
    "def construct_trades_url(params):\n",
    "    base_url = 'https://api.kpler.com/v2/cargo/trades?'\n",
    "    for key, value in params.items():\n",
    "        if value:\n",
    "            if isinstance(value, list):\n",
    "                for item in value:\n",
    "                    base_url += f\"&{key}[]={item.strip()}\"\n",
    "            else:\n",
    "                base_url += f\"&{key}={value}\"\n",
    "    return base_url\n",
    "\n",
    "# Fetch Trades Data\n",
    "def fetch_trades_data(url, headers):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        print(\"Trades API call successful.\")\n",
    "        json_response_trades = response.json()\n",
    "        return json_response_trades\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: {response.text}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the json response to a Pandas dataframe and keep only the data fields of interest\n",
    "def normalize_trades_data(data):\n",
    "    df_trades = pd.json_normalize(\n",
    "        data, \n",
    "        record_path=['vessels'],\n",
    "        meta=['id', 'status',\n",
    "              ['vessel', 'imo'],\n",
    "              ['vessel', 'name'],    \n",
    "              ['vessel', 'type'],\n",
    "              ['vessel', 'cargoType'],   \n",
    "              ['origin', 'zone', 'name'],\n",
    "              ['origin', 'arrival'],\n",
    "              ['origin', 'departure'],\n",
    "              ['destination', 'zone', 'name'],\n",
    "              ['destination', 'arrival'],\n",
    "              ['destination', 'departure'],\n",
    "        ],\n",
    "        sep='_',\n",
    "        errors='ignore',\n",
    "        meta_prefix='trades_'  # Prefix all meta fields to avoid conflicts\n",
    "    )\n",
    "\n",
    "    # Keep only the columns of interest\n",
    "    trades = df_trades[['voyageId', 'vessel_id', 'vessel_name', 'vessel_mmsi', 'vessel_imo', 'vessel_deadWeightTonnage',\n",
    "                        'vessel_capacity', 'trades_id', 'trades_status',\n",
    "                        'trades_origin_zone_name', 'trades_destination_zone_name']]\n",
    "\n",
    "    # Keep only rows where 'imo' is not NaN\n",
    "    trades_filtered = trades[trades['vessel_imo'].notna() & (trades['vessel_imo'] != '')]\n",
    "\n",
    "    return trades_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Compliance Data\n",
    "def fetch_compliance_data(imo_list, start_date, end_date, headers):\n",
    "    compliance_url = 'https://api.kpler.com/v2/compliance/vessel-risks'\n",
    "    \n",
    "    # Compliance API expects the request body as a list of IMO numbers\n",
    "    body = imo_list  # Directly pass the IMO list as the request body\n",
    "    \n",
    "    # API Call\n",
    "    response_compliance = requests.post(compliance_url, headers=headers, json=body)\n",
    "    \n",
    "    if response_compliance.status_code == 200:\n",
    "        print(\"Compliance API call successful.\")\n",
    "        return response_compliance.json()\n",
    "    else:\n",
    "        print(f\"Error {response_compliance.status_code}: {response_compliance.text}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Array Handling\n",
    "def expand_and_select_columns(df, column_name, selected_keys, prefix):\n",
    "    \"\"\"\n",
    "    Expands a column containing a list of dictionaries into separate columns for selected keys.\n",
    "    Converts Unix timestamps to 'YYYY/MM/DD HH:MM:SS' for specific keys like 'startDate' and 'endDate'.\n",
    "    Replaces empty or missing values with NaN.\n",
    "    \"\"\"\n",
    "    if column_name in df.columns:\n",
    "        # Ensure the column contains lists (drop invalid entries)\n",
    "        valid_entries = df[column_name].apply(lambda x: isinstance(x, list))\n",
    "        if not valid_entries.all():\n",
    "            print(f\"Non-list entries found in column {column_name}. Converting to empty lists where necessary.\")\n",
    "            df[column_name] = df[column_name].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "        # Initialize a temporary DataFrame to hold expanded data\n",
    "        temp_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through each selected key\n",
    "        for key in selected_keys:\n",
    "            # Extract the values for the current key\n",
    "            extracted = df[column_name].apply(lambda x: [d.get(key, None) for d in x] if isinstance(x, list) else [])\n",
    "            \n",
    "            # Convert Unix timestamps to readable format if the key is 'startDate' or 'endDate'\n",
    "            if key in ['startDate', 'endDate']:\n",
    "                extracted = extracted.apply(lambda x: [convert_unix_to_datetime(ts) for ts in x])\n",
    "\n",
    "            # Replace empty or None values with NaN\n",
    "            extracted = extracted.apply(lambda x: [item if item is not None else np.nan for item in x])\n",
    "\n",
    "            # Create new columns for each value in the list\n",
    "            max_len = extracted.apply(len).max()  # Find the maximum number of entries for the key\n",
    "            for i in range(max_len):\n",
    "                temp_df[f\"{prefix}_{key}_{i+1}\"] = extracted.apply(lambda x: x[i] if i < len(x) else np.nan)\n",
    "\n",
    "        # Concatenate the new columns back to the main DataFrame\n",
    "        df = pd.concat([df, temp_df], axis=1)\n",
    "\n",
    "        # Drop the original array column\n",
    "        df = df.drop(columns=[column_name])\n",
    "    else:\n",
    "        print(f\"Column {column_name} not found in the DataFrame.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def convert_unix_to_datetime(unix_timestamp):\n",
    "    \"\"\"\n",
    "    Converts a Unix timestamp to 'YYYY/MM/DD HH:MM:SS' format.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if unix_timestamp is not None:\n",
    "            return datetime.utcfromtimestamp(int(unix_timestamp)).strftime('%Y/%m/%d %H:%M:%S')\n",
    "        return np.nan  # Return NaN if the timestamp is None\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting timestamp: {unix_timestamp}. Error: {e}\")\n",
    "        return np.nan  # Return NaN in case of an error\n",
    "\n",
    "# Normalize the data\n",
    "def normalize_compliance_data(data):\n",
    "    \"\"\"\n",
    "    Normalizes compliance data, expands nested fields, and allows for key selection from arrays.\n",
    "    \"\"\"\n",
    "    # Flatten the JSON data\n",
    "    df_compliance = pd.json_normalize(data, sep='_')\n",
    "\n",
    "    # Define the required columns\n",
    "    required_columns = [\n",
    "        'vessel_imo', 'vessel_mmsi', 'vessel_callsign', 'vessel_shipname',\n",
    "        'vessel_flag', 'vessel_countryCode', 'vessel_typeName',\n",
    "        'vessel_typeSummary', 'vessel_particulars_gt', 'vessel_particulars_yob',\n",
    "        'vessel_vesselCompanies',\n",
    "        'compliance_sanctionRisks_sanctionedVessels_isSanctioned',\n",
    "        'compliance_sanctionRisks_sanctionedVessels_historicalData',\n",
    "        'compliance_sanctionRisks_sanctionedCompanies_isSanctioned',\n",
    "        'compliance_sanctionRisks_sanctionedCompanies_companies',\n",
    "        'compliance_sanctionRisks_sanctionedCompanies_historicalData',\n",
    "    ]\n",
    "\n",
    "    # Check for missing columns\n",
    "    missing_columns = [col for col in required_columns if col not in df_compliance.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"Warning: Missing columns - {missing_columns}\")\n",
    "\n",
    "    # Select only available columns\n",
    "    compliance = df_compliance[[col for col in required_columns if col in df_compliance.columns]]\n",
    "\n",
    "    # Expand nested arrays and select keys\n",
    "    compliance = expand_and_select_columns(\n",
    "        compliance,\n",
    "        column_name='vessel_vesselCompanies',\n",
    "        selected_keys=['typeName', 'name'],\n",
    "        prefix='vessel_vesselCompanies'\n",
    "    )\n",
    "\n",
    "    compliance = expand_and_select_columns(\n",
    "        compliance,\n",
    "        column_name='compliance_sanctionRisks_sanctionedCompanies_companies',\n",
    "        selected_keys=['name'],\n",
    "        prefix='sanctionedCompanies_companies'\n",
    "    )\n",
    "\n",
    "    compliance = expand_and_select_columns(\n",
    "        compliance,\n",
    "        column_name='compliance_sanctionRisks_sanctionedVessels_historicalData',\n",
    "        selected_keys=['name', 'startDate', 'endDate'],\n",
    "        prefix='sanctionedVessels_historicalData'\n",
    "    )\n",
    "\n",
    "    compliance = expand_and_select_columns(\n",
    "        compliance,\n",
    "        column_name='compliance_sanctionRisks_sanctionedCompanies_historicalData',\n",
    "        selected_keys=['name', 'startDate', 'endDate'],\n",
    "        prefix='sanctionedCompanies_historicalData'\n",
    "    )\n",
    "\n",
    "    # Return the processed DataFrame\n",
    "    return compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the positions for the IMO numbers using the MarineTraffic API\n",
    "def fetch_ais_data(api_key, imo_list, timespan=1440, buffer_time=120):\n",
    "    \"\"\"\n",
    "    Fetch AIS data for a list of IMO numbers from the MarineTraffic API.\n",
    "\n",
    "    Parameters:\n",
    "    - api_key (str): The API key for authentication.\n",
    "    - imo_list (list): List of IMO numbers to fetch data for.\n",
    "    - timespan (int): Timespan in minutes for the data (default is 1440 minutes, i.e., 24 hours).\n",
    "    - buffer_time (int): Time in seconds to wait between API calls (default is 1 second).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing the filtered AIS data for the IMO numbers.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Starting AIS data retrieval...\")\n",
    "    url_template = f'https://services.marinetraffic.com/api/exportvessel/{api_key}/v:6/timespan:{timespan}/imo:{{imo}}/protocol:jsono'\n",
    "    all_ais_data = []\n",
    "    \n",
    "    for idx, imo in enumerate(imo_list, start=1):\n",
    "        print(f\"[{idx}/{len(imo_list)}] Fetching AIS data for IMO: {imo}...\")\n",
    "        try:\n",
    "            # API call\n",
    "            response = requests.get(url_template.format(imo=imo))\n",
    "            if response.ok:\n",
    "                data = response.json()\n",
    "                if isinstance(data, list):\n",
    "                    for record in data:\n",
    "                        record['IMO'] = imo  # Add IMO to each record\n",
    "                        all_ais_data.append(record)\n",
    "                    print(f\"SUCCESS: Data fetched and processed for IMO {imo}.\")\n",
    "                else:\n",
    "                    print(f\"WARNING: Unexpected data format for IMO {imo}.\")\n",
    "            else:\n",
    "                print(f\"ERROR: Failed to fetch data for IMO {imo}. Status: {response.status_code} {response.reason}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Exception occurred for IMO {imo}: {e}\")\n",
    "        \n",
    "        # Add a buffer time (in seconds) between API calls\n",
    "        if idx < len(imo_list):  # Avoid waiting after the last call\n",
    "            print(f\"Waiting for {buffer_time} seconds before the next call...\")\n",
    "            time.sleep(buffer_time)\n",
    "\n",
    "    # Check if any data was collected\n",
    "    if all_ais_data:\n",
    "        # Create DataFrame\n",
    "        df_ais = pd.DataFrame(all_ais_data)\n",
    "        # Retain only the desired columns: LAT, LON, and TIMESTAMP\n",
    "        df_ais = df_ais[['IMO', 'LAT', 'LON', 'TIMESTAMP']] if {'IMO', 'LAT', 'LON', 'TIMESTAMP'}.issubset(df_ais.columns) else pd.DataFrame(columns=['IMO', 'LAT', 'LON', 'TIMESTAMP'])\n",
    "        print(\"AIS data retrieval complete.\")\n",
    "        return df_ais\n",
    "    else:\n",
    "        print(\"No AIS data retrieved.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the Two Data Frames Using the VoyageId and Include AIS Data\n",
    "def combine_filtered(trades_filtered, compliance_filtered, ais_data):\n",
    "    # Handle NaN values in 'vessel_imo' columns before conversion\n",
    "    trades_filtered['vessel_imo'] = trades_filtered['vessel_imo'].fillna(np.nan).astype(int)\n",
    "    compliance_filtered['vessel_imo'] = compliance_filtered['vessel_imo'].fillna(np.nan).astype(int)\n",
    "    ais_data['IMO'] = ais_data['IMO'].fillna(np.nan).astype(int)\n",
    "\n",
    "    # Merge the trades and compliance data\n",
    "    df_merged = pd.merge(\n",
    "        trades_filtered,\n",
    "        compliance_filtered, \n",
    "        how='outer',  # 'outer' to keep all records\n",
    "        left_on='vessel_imo',\n",
    "        right_on='vessel_imo',\n",
    "        suffixes=('_trade', '_compliance')\n",
    "    )\n",
    "    \n",
    "    # Merge the result with AIS data\n",
    "    df_final = pd.merge(\n",
    "        df_merged,\n",
    "        ais_data,\n",
    "        how='left',  # Use 'left' to keep all records from trades/compliance\n",
    "        left_on='vessel_imo',\n",
    "        right_on='IMO'\n",
    "    )\n",
    "\n",
    "    # Drop the 'IMO' column from AIS data\n",
    "    df_final = df_final.drop(columns=['IMO'], errors='ignore')\n",
    "    \n",
    "    # Fill NaN values explicitly where applicable (if needed for other columns)\n",
    "    df_final = df_final.fillna(np.nan)\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    try:\n",
    "        # Step 1: Retrieve API parameters\n",
    "        params = get_api_params()\n",
    "\n",
    "        # Step 2: Fetch Trades data\n",
    "        trades_url = construct_trades_url(params)\n",
    "        headers = {'Authorization': f'Basic {API_KEY}', 'Accept': 'application/json'}\n",
    "        trades_data = fetch_trades_data(trades_url, headers)\n",
    "\n",
    "        if trades_data:\n",
    "            df_trades = normalize_trades_data(trades_data)\n",
    "            trades_file_path = 'E:/json_response_trades.csv'\n",
    "            df_trades.to_csv(trades_file_path, index=False)\n",
    "            print(f\"Trades data saved to '{trades_file_path}'\")\n",
    "\n",
    "            # Extract distinct IMO numbers\n",
    "            distinct_imo_list = (\n",
    "                df_trades['vessel_imo']\n",
    "                .dropna()\n",
    "                .unique()\n",
    "                .astype(int)\n",
    "                .tolist()\n",
    "            )\n",
    "\n",
    "            # Step 3: Fetch Compliance data\n",
    "            compliance_data = fetch_compliance_data(distinct_imo_list, DEFAULT_ORIGIN_DATE_START, DEFAULT_ORIGIN_DATE_END, headers)\n",
    "\n",
    "            if compliance_data:\n",
    "                df_compliance = normalize_compliance_data(compliance_data)\n",
    "                compliance_file_path = 'E:/compliance_data.csv'\n",
    "                df_compliance.to_csv(compliance_file_path, index=False)\n",
    "                print(f\"Compliance data saved to '{compliance_file_path}'\")\n",
    "\n",
    "                # Step 4: Fetch AIS data\n",
    "                df_ais = fetch_ais_data(API_KEY_MT, distinct_imo_list, timespan=1440)\n",
    "                if df_ais is not None:\n",
    "                    ais_file_path = 'E:/ais_data.csv'\n",
    "                    df_ais.to_csv(ais_file_path, index=False)\n",
    "                    print(f\"AIS data saved to '{ais_file_path}'\")\n",
    "\n",
    "                    # Step 5: Merge all data\n",
    "                    merged_data = combine_filtered(df_trades, df_compliance, df_ais)\n",
    "                    merged_file_path = 'E:/merged_trades_compliance_ais_data.csv'\n",
    "                    merged_data.to_csv(merged_file_path, index=False)\n",
    "                    print(f\"Merged data saved to '{merged_file_path}'\")\n",
    "                else:\n",
    "                    print(\"Failed to fetch AIS data.\")\n",
    "            else:\n",
    "                print(\"Failed to fetch Compliance data.\")\n",
    "        else:\n",
    "            print(\"Failed to fetch Trades data.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Execute Main\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
