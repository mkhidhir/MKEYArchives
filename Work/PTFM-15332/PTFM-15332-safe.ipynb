{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3b5052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# Section 1: Imports and Configuration\n",
    "# ================================================\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "# API Keys (Replace with your actual values)\n",
    "PORTCALL_API_KEYS = {\n",
    "    \"US\": \"your_us_api_key_here\",\n",
    "    \"SEA\": \"your_sea_api_key_here\",\n",
    "    \"ES\": \"your_spain_api_key_here\",\n",
    "    \"UK\": \"your_uk_api_key_here\",\n",
    "    \"CN\": \"your_china_api_key_here\",\n",
    "    # Add more country codes and keys as needed\n",
    "}\n",
    "\n",
    "# Power Tools API Keys and Fleet ID\n",
    "CLEARFLEET_API_KEY = \"\"\n",
    "SETFLEET_API_KEY = \"\"\n",
    "FLEET_ID = # Add your fleet ID here\n",
    "\n",
    "# New API key for fleet portcalls (Step 4)\n",
    "FLEET_PORTCALL_API_KEY = \"\" #add your fleet portcall api key here\n",
    "\n",
    "# Set API endpoints and keys\n",
    "GRAPHQL_URL = \"https://api.kpler.marinetraffic.com/v2/vessels/graphql\"\n",
    "GRAPHQL_API_KEY = \"\" # Add your GraphQL API key here\n",
    "\n",
    "MARKET_OPTIONS = [\n",
    "    \"CONTAINER SHIPS\", \"DRY BREAKBULK\", \"DRY BULK\", \"LNG CARRIERS\",\n",
    "    \"LPG CARRIERS\", \"WET BULK\", \"PASSENGER SHIPS\", \"OFFSHORE/RIGS\",\n",
    "    \"RO/RO\", \"SUPPORTING VESSELS\", \"PLEASURE CRAFT\", \"FISHING\", \"OTHER MARKETS\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ba9e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# Section 2: Helper Input Functions\n",
    "# ================================================\n",
    "def safe_input(prompt, validate_func=None, allowed_values=None):\n",
    "    while True:\n",
    "        user_input = input(prompt).strip()\n",
    "        if user_input.upper() == 'ESC':\n",
    "            raise KeyboardInterrupt(\"User pressed ESC - exiting.\")\n",
    "        if not user_input:\n",
    "            print(\"Input cannot be empty. Please try again or type ESC to exit.\")\n",
    "            continue\n",
    "        if allowed_values and user_input.upper() not in allowed_values:\n",
    "            print(f\"Invalid option. Allowed: {', '.join(allowed_values)}\")\n",
    "            continue\n",
    "        if validate_func:\n",
    "            try:\n",
    "                return validate_func(user_input)\n",
    "            except Exception as e:\n",
    "                print(f\"Invalid input: {e}\")\n",
    "                continue\n",
    "        else:\n",
    "            return user_input\n",
    "\n",
    "def get_user_portcall_input():\n",
    "    allowed_regions = set(PORTCALL_API_KEYS.keys())\n",
    "    region = safe_input(f\"Select region/country code {sorted(allowed_regions)}: \", allowed_values=allowed_regions).upper()\n",
    "    \n",
    "    def validate_date(d): return datetime.strptime(d, \"%Y-%m-%d\")\n",
    "    fromdate = safe_input(\"Enter fromdate (YYYY-MM-DD): \", validate_func=validate_date)\n",
    "    todate = safe_input(\"Enter todate (YYYY-MM-DD): \", validate_func=validate_date)\n",
    "    \n",
    "    from_date = fromdate.strftime('%Y-%m-%d 00:00:00')\n",
    "    to_date = todate.strftime('%Y-%m-%d 23:59:00')\n",
    "    \n",
    "    print(\"\\nAvailable Market Types:\")\n",
    "    for i, m in enumerate(MARKET_OPTIONS, 1): print(f\"{i}. {m}\")\n",
    "    \n",
    "    def validate_markets(m):\n",
    "        indices = [int(i.strip()) - 1 for i in m.split(\",\")]\n",
    "        return [MARKET_OPTIONS[i] for i in indices]\n",
    "    \n",
    "    market_list = safe_input(\"Select market(s) by number: \", validate_func=validate_markets)\n",
    "    movetype = 0 # or allow user input as before\n",
    "    \n",
    "    return region, from_date, to_date, market_list, movetype\n",
    "\n",
    "def split_date_range(start_date, end_date, max_days=190):\n",
    "    \"\"\"\n",
    "    Splits a date range into chunks of max_days.\n",
    "    Returns a list of (chunk_start, chunk_end) tuples.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current_start = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "    while current_start < end_date:\n",
    "        current_end = min(current_start + timedelta(days=max_days-1), end_date)\n",
    "        chunks.append((current_start.strftime('%Y-%m-%d %H:%M:%S'), current_end.strftime('%Y-%m-%d %H:%M:%S')))\n",
    "        current_start = current_end + timedelta(seconds=1)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c80e6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# Section 3: PortCall API Functions\n",
    "# ================================================\n",
    "def build_portcall_url(api_key, fromdate, todate, msgtype='extended', market=None, movetype=0):\n",
    "    url = (\n",
    "        f\"https://services.marinetraffic.com/api/portcalls/{api_key}/v:6/\"\n",
    "        f\"fromdate:{fromdate.replace(' ', '%20')}/\"\n",
    "        f\"todate:{todate.replace(' ', '%20')}/\"\n",
    "        f\"msgtype:{msgtype}/protocol:jsono\"\n",
    "    )\n",
    "    if market: url += f\"/market:{market.replace(' ', '%20')}\"\n",
    "    if movetype is not None: url += f\"/movetype:{movetype}\"\n",
    "    return url\n",
    "\n",
    "def fetch_portcall_data_single(region, from_date, to_date, market_list, movetype):\n",
    "    api_key = PORTCALL_API_KEYS[region]\n",
    "    all_data = []\n",
    "    for market in market_list:\n",
    "        if movetype in [0, 1]:\n",
    "            url = build_portcall_url(api_key, from_date, to_date, market=market, movetype=movetype)\n",
    "            all_data.extend(requests.get(url).json())\n",
    "        else:\n",
    "            for mt in [0, 1]:\n",
    "                url = build_portcall_url(api_key, from_date, to_date, market=market, movetype=mt)\n",
    "                all_data.extend(requests.get(url).json())\n",
    "    return all_data\n",
    "\n",
    "def fetch_portcall_data(region, from_date, to_date, market_list, movetype):\n",
    "    all_data = []\n",
    "    date_chunks = split_date_range(from_date, to_date, max_days=190)\n",
    "    for chunk_start, chunk_end in date_chunks:\n",
    "        print(f\"Fetching port calls from {chunk_start} to {chunk_end}...\")\n",
    "        data = fetch_portcall_data_single(region, chunk_start, chunk_end, market_list, movetype)\n",
    "        if data:\n",
    "            all_data.extend(data)\n",
    "    return all_data\n",
    "\n",
    "def normalize_portcall_data(data):\n",
    "    df = pd.json_normalize(data)\n",
    "    status_map = {0: \"NA\", 1: \"In Ballast\", 2: \"Partially Laden\", 3: \"Fully Laden\"}\n",
    "    df = df.rename(columns={\n",
    "        'IMO': 'IMO', 'SHIPNAME': 'SHIPNAME', 'TIMESTAMP_UTC': 'TIMESTAMP_UTC',\n",
    "        'MOVE_TYPE': 'MOVE_TYPE', 'PORT_ID': 'PORT_ID', 'PORT_NAME': 'PORT_NAME',\n",
    "        'SHIP_ID': 'SHIP_ID', 'LOAD_STATUS': 'LOAD_STATUS'\n",
    "    })\n",
    "    df['LOAD_STATUS'] = df['LOAD_STATUS'].fillna(0).astype(int).map(status_map).fillna(\"Unknown\")\n",
    "    # Exclude anchorages and canals\n",
    "    df = df[~df['PORT_NAME'].str.lower().str.contains('canal|anch', na=False)]\n",
    "    return df[['IMO', 'SHIPNAME', 'TIMESTAMP_UTC', 'MOVE_TYPE', 'PORT_ID', 'PORT_NAME', 'SHIP_ID', 'LOAD_STATUS']]\n",
    "\n",
    "def save_df_to_csv(df, prefix=\"portcalls\"):\n",
    "    filename = f\"{prefix}_{datetime.utcnow().strftime('%Y%m%d_%H%M')}.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cec723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# Section 4: ClearFleet API (Clears fleet list)\n",
    "# ================================================\n",
    "def build_clearfleet_url(api_key, fleet_id):\n",
    "    \"\"\"Build URL for ClearFleet API.\"\"\"\n",
    "    return f\"https://services.marinetraffic.com/api/clearfleet/{api_key}/fleet_id:{fleet_id}\"\n",
    "\n",
    "def is_clearfleet_successful(xml_text):\n",
    "    \"\"\"Parse XML response and return True if fleet cleared successfully.\"\"\"\n",
    "    try:\n",
    "        root = ET.fromstring(xml_text)\n",
    "        fleet = root.find(\".//FLEET\")\n",
    "        if fleet is not None and fleet.attrib.get(\"DELETE\") == \"1\":\n",
    "            return True\n",
    "        return False\n",
    "    except ET.ParseError:\n",
    "        return False\n",
    "\n",
    "def clear_fleet(api_key, fleet_id):\n",
    "    \"\"\"Call ClearFleet API to clear a fleet. Returns success status.\"\"\"\n",
    "    url = build_clearfleet_url(api_key, fleet_id)\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=10)\n",
    "        resp.raise_for_status()\n",
    "        if is_clearfleet_successful(resp.text):\n",
    "            print(\"✅ Fleet cleared successfully.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"❌ Unexpected response:\\n{resp.text}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to clear fleet: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b912d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# Section 5: SetFleet API – Add IMOs One-by-One\n",
    "# ================================================\n",
    "def build_setfleet_batch_url(api_key, fleet_id, imo_list):\n",
    "    \"\"\"Build the SetFleet API URL to add multiple vessels by IMO (batch).\"\"\"\n",
    "    imo_str = \",\".join(str(imo) for imo in imo_list)\n",
    "    return f\"https://services.marinetraffic.com/api/setfleet/{api_key}/fleet_id:{fleet_id}/imo:{imo_str}/active:1\"\n",
    "\n",
    "def add_vessels_to_fleet(api_key, fleet_id, imo_list, batch_size=300):\n",
    "    \"\"\"Add vessels to a MarineTraffic fleet in batches using IMO. Returns success & failed lists.\"\"\"\n",
    "    if not imo_list:\n",
    "        print(\"No IMOs provided to add to fleet.\")\n",
    "        return [], []\n",
    "\n",
    "    success_ids = []\n",
    "    failed_ids = []\n",
    "\n",
    "    print(f\"\\n== Step 3: SetFleet (Add Vessels in Batches of {batch_size}) ===\")\n",
    "    print(f\"Adding {len(imo_list)} vessels to fleet {fleet_id}...\\n\")\n",
    "\n",
    "    for batch_num, start in enumerate(range(0, len(imo_list), batch_size), 1):\n",
    "        batch = imo_list[start:start+batch_size]\n",
    "        url = build_setfleet_batch_url(api_key, fleet_id, batch)\n",
    "        try:\n",
    "            resp = requests.get(url, timeout=30)\n",
    "            resp.raise_for_status()\n",
    "            root = ET.fromstring(resp.text)\n",
    "            # Find all ERROR nodes\n",
    "            error_nodes = root.findall(\".//ERROR\")\n",
    "            error_imos = set()\n",
    "            for error in error_nodes:\n",
    "                imo = error.attrib.get(\"IMO\")\n",
    "                error_imos.add(imo)\n",
    "                print(f\"[Batch {batch_num}] ❌ IMO {imo} failed. Error: {error.attrib.get('DESCRIPTION', '')}\")\n",
    "                failed_ids.append((imo, error.attrib.get(\"DESCRIPTION\", \"\")))\n",
    "            # Mark all other IMOs as success\n",
    "            for imo in batch:\n",
    "                if imo not in error_imos:\n",
    "                    print(f\"[Batch {batch_num}] ✅ IMO {imo} added.\")\n",
    "                    success_ids.append(imo)\n",
    "        except Exception as e:\n",
    "            print(f\"[Batch {batch_num}] ❌ Batch error: {e}\")\n",
    "            for imo in batch:\n",
    "                failed_ids.append((imo, str(e)))\n",
    "        time.sleep(5)  # Add 5-second buffer between API calls\n",
    "\n",
    "    print(f\"\\nAdded {len(success_ids)} vessels successfully; {len(failed_ids)} failed.\")\n",
    "    return success_ids, failed_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9725f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# Section 6: Step 4 - Fleet Port Call API (Filtered All Records)\n",
    "# ================================================\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "MAX_DURATION_DAYS = 190\n",
    "\n",
    "def build_fleet_portcall_url(api_key, fromdate, todate):\n",
    "    return (\n",
    "        f\"https://services.marinetraffic.com/api/portcalls/{api_key}/v:6/\"\n",
    "        f\"fromdate:{fromdate.replace(' ', '%20')}/\"\n",
    "        f\"todate:{todate.replace(' ', '%20')}/\"\n",
    "        f\"msgtype:extended/protocol:jsono\"\n",
    "    )\n",
    "\n",
    "def fetch_fleet_portcall_data(api_key, fromdate, todate):\n",
    "    url = build_fleet_portcall_url(api_key, fromdate, todate)\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()\n",
    "\n",
    "def normalize_fleet_portcalls(data):\n",
    "    df = pd.json_normalize(data)\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df = df.rename(columns={\n",
    "        'IMO': 'IMO',\n",
    "        'SHIPNAME': 'SHIPNAME',\n",
    "        'TIMESTAMP_UTC': 'TIMESTAMP_UTC',\n",
    "        'MOVE_TYPE': 'MOVE_TYPE',\n",
    "        'PORT_ID': 'PORT_ID',\n",
    "        'PORT_NAME': 'PORT_NAME',\n",
    "        'PORT_COUNTRY_CODE': 'PORT_COUNTRY_CODE',\n",
    "        'SHIP_ID': 'SHIP_ID'\n",
    "    })\n",
    "    df['TIMESTAMP_UTC'] = pd.to_datetime(df['TIMESTAMP_UTC'])\n",
    "    # Exclude anchorages and canals\n",
    "    df = df[~df['PORT_NAME'].str.lower().str.contains('canal|anch', na=False)]\n",
    "    return df[['IMO', 'SHIPNAME', 'SHIP_ID', 'TIMESTAMP_UTC', 'MOVE_TYPE', 'PORT_ID', 'PORT_NAME', 'PORT_COUNTRY_CODE']]\n",
    "\n",
    "def save_fleet_portcalls_single(api_key, fromdate_fleet, todate_fleet, region=None):\n",
    "    all_data = []\n",
    "    current_start = fromdate_fleet\n",
    "    while current_start < todate_fleet:\n",
    "        current_end = min(current_start + timedelta(days=MAX_DURATION_DAYS), todate_fleet)\n",
    "        print(f\"Fetching fleet port calls for region {region} from {current_start} to {current_end}...\")\n",
    "        data = fetch_fleet_portcall_data(\n",
    "            api_key,\n",
    "            current_start.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            current_end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        )\n",
    "        all_data.extend(data)\n",
    "        current_start = current_end + timedelta(seconds=1)\n",
    "    df_fleet = normalize_fleet_portcalls(all_data)\n",
    "    return df_fleet\n",
    "\n",
    "def save_fleet_portcalls(api_key, fromdate, todate, region=None):\n",
    "    region_str = region.lower() if region else \"unknown\"\n",
    "    print(f\"\\n=== Step 4: Fleet Port Call API - All Records for {region_str.upper()} (Excl. region, Departures Only) ===\")\n",
    "    fromdate_fleet = datetime.strptime(fromdate, '%Y-%m-%d %H:%M:%S') - relativedelta(months=2)\n",
    "    todate_fleet = datetime.strptime(todate, '%Y-%m-%d %H:%M:%S') + relativedelta(months=1)\n",
    "\n",
    "    all_dfs = []\n",
    "    date_chunks = split_date_range(fromdate_fleet, todate_fleet, max_days=MAX_DURATION_DAYS)\n",
    "    for chunk_start, chunk_end in date_chunks:\n",
    "        df = save_fleet_portcalls_single(api_key, pd.to_datetime(chunk_start), pd.to_datetime(chunk_end), region=region_str)\n",
    "        if df is not None and not df.empty:\n",
    "            all_dfs.append(df)\n",
    "    if all_dfs:\n",
    "        df_full = pd.concat(all_dfs, ignore_index=True)\n",
    "        filename = f\"{region_str}_fleet_portcalls_filtered_{datetime.utcnow().strftime('%Y%m%d_%H%M')}.csv\"\n",
    "        df_full.to_csv(filename, index=False)\n",
    "        print(f\"✅ Saved fleet port calls to {filename}\")\n",
    "        return filename\n",
    "    else:\n",
    "        print(f\"No fleet port call data found for region {region_str}.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d100aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# Section 7A: Last Departure & Next Arrival (multi-country, robust)\n",
    "# ================================================\n",
    "\n",
    "# Define exclusion lists for each region/country code\n",
    "EXCLUDE_COUNTRIES = {\n",
    "    \"US\": [\"US\"],\n",
    "    \"SEA\": [\"SG\", \"MY\", \"ID\", \"TH\", \"VN\", \"PH\", \"KH\", \"MM\", \"BN\", \"LA\", \"TL\"],\n",
    "    \"ES\": [\"ES\"],\n",
    "    \"UK\": [\"GB\", \"UK\"],\n",
    "    \"CN\": [\"CN\"],\n",
    "    # Add more country codes and their exclusions as needed\n",
    "}\n",
    "\n",
    "def enrich_portcalls_with_last_departure(portcalls_df, fleet_df):\n",
    "    \"\"\"\n",
    "    Enriches portcalls_df with last non-region departure info from fleet_df.\n",
    "    Adds LAST_PORT_ID, LAST_PORT, LAST_PORT_COUNTRY_CODE, LAST_PORT_TIMESTAMP columns.\n",
    "    \"\"\"\n",
    "    portcalls_df = portcalls_df.copy()\n",
    "    fleet_df = fleet_df.copy()\n",
    "\n",
    "    portcalls_df[\"LAST_PORT_ID\"] = pd.NA\n",
    "    portcalls_df[\"LAST_PORT\"] = pd.NA\n",
    "    portcalls_df[\"LAST_PORT_COUNTRY_CODE\"] = pd.NA\n",
    "    portcalls_df[\"LAST_PORT_TIMESTAMP\"] = pd.NaT\n",
    "\n",
    "    fleet_df[\"TIMESTAMP_UTC\"] = pd.to_datetime(fleet_df[\"TIMESTAMP_UTC\"], errors='coerce', utc=True)\n",
    "    portcalls_df[\"TIMESTAMP_UTC\"] = pd.to_datetime(portcalls_df[\"TIMESTAMP_UTC\"], errors='coerce', utc=True)\n",
    "\n",
    "    group_col = \"SHIP_ID\" if \"SHIP_ID\" in portcalls_df.columns and \"SHIP_ID\" in fleet_df.columns else \"IMO\"\n",
    "    portcalls_df[group_col] = portcalls_df[group_col].astype(str)\n",
    "    fleet_df[group_col] = fleet_df[group_col].astype(str)\n",
    "    fleet_grouped = fleet_df.groupby(group_col)\n",
    "\n",
    "    for idx, row in portcalls_df.iterrows():\n",
    "        ship_key = str(row[group_col])\n",
    "        call_time = row[\"TIMESTAMP_UTC\"]\n",
    "\n",
    "        if ship_key in fleet_grouped.groups:\n",
    "            vessel_calls = fleet_grouped.get_group(ship_key)\n",
    "            prior_departures = vessel_calls[\n",
    "                (vessel_calls[\"MOVE_TYPE\"] == 1) &\n",
    "                (vessel_calls[\"TIMESTAMP_UTC\"] < call_time)\n",
    "            ]\n",
    "            if not prior_departures.empty:\n",
    "                latest_dep = prior_departures.sort_values(\"TIMESTAMP_UTC\").iloc[-1]\n",
    "                portcalls_df.loc[idx, \"LAST_PORT_ID\"] = latest_dep.get(\"PORT_ID\", pd.NA)\n",
    "                portcalls_df.loc[idx, \"LAST_PORT\"] = latest_dep.get(\"PORT_NAME\", pd.NA)\n",
    "                portcalls_df.loc[idx, \"LAST_PORT_COUNTRY_CODE\"] = latest_dep.get(\"PORT_COUNTRY_CODE\", pd.NA)\n",
    "                portcalls_df.loc[idx, \"LAST_PORT_TIMESTAMP\"] = pd.to_datetime(latest_dep[\"TIMESTAMP_UTC\"]).tz_localize(None)\n",
    "\n",
    "    return portcalls_df\n",
    "\n",
    "def enrich_portcalls_with_last_departure_region(portcalls_df, fleet_df, region):\n",
    "    \"\"\"\n",
    "    Filters fleet_df to exclude region ports, then enriches portcalls_df with last departure info.\n",
    "    \"\"\"\n",
    "    region = region.strip().upper()\n",
    "    exclude_countries = EXCLUDE_COUNTRIES.get(region, [])\n",
    "    fleet_df = fleet_df.copy()\n",
    "    if exclude_countries and 'PORT_COUNTRY_CODE' in fleet_df.columns:\n",
    "        fleet_df = fleet_df[\n",
    "            ~fleet_df['PORT_COUNTRY_CODE'].str.upper().isin(exclude_countries)\n",
    "        ]\n",
    "    return enrich_portcalls_with_last_departure(portcalls_df, fleet_df)\n",
    "\n",
    "def enrich_portcalls_with_next_nonregion_arrival(portcalls_df, fleet_df, region):\n",
    "    \"\"\"\n",
    "    Enriches portcalls_df with next non-region arrival info from fleet_df.\n",
    "    Adds NEXT_PORT_ID, NEXT_PORT, NEXT_PORT_COUNTRY, NEXT_PORT_TIMESTAMP columns.\n",
    "    \"\"\"\n",
    "    portcalls_df = portcalls_df.copy()\n",
    "    fleet_df = fleet_df.copy()\n",
    "\n",
    "    portcalls_df[\"TIMESTAMP_UTC\"] = pd.to_datetime(portcalls_df[\"TIMESTAMP_UTC\"], utc=True)\n",
    "    fleet_df[\"TIMESTAMP_UTC\"] = pd.to_datetime(fleet_df[\"TIMESTAMP_UTC\"], utc=True)\n",
    "\n",
    "    region = region.strip().upper()\n",
    "    exclude_countries = EXCLUDE_COUNTRIES.get(region, [])\n",
    "\n",
    "    # Filter out arrivals in the region and only arrivals (MOVE_TYPE == 0)\n",
    "    df_arrivals = fleet_df[\n",
    "        (~fleet_df[\"PORT_COUNTRY_CODE\"].str.upper().isin(exclude_countries)) &\n",
    "        (fleet_df[\"MOVE_TYPE\"] == 0)\n",
    "    ].copy()\n",
    "\n",
    "    portcalls_df[\"NEXT_PORT_ID\"] = pd.NA\n",
    "    portcalls_df[\"NEXT_PORT\"] = pd.NA\n",
    "    portcalls_df[\"NEXT_PORT_COUNTRY\"] = pd.NA\n",
    "    portcalls_df[\"NEXT_PORT_TIMESTAMP\"] = pd.NaT\n",
    "\n",
    "    # Use SHIP_ID for grouping if available, else fallback to IMO\n",
    "    group_col = \"SHIP_ID\" if \"SHIP_ID\" in portcalls_df.columns and \"SHIP_ID\" in df_arrivals.columns else \"IMO\"\n",
    "    portcalls_df[group_col] = portcalls_df[group_col].astype(str)\n",
    "    df_arrivals[group_col] = df_arrivals[group_col].astype(str)\n",
    "    arrivals_grouped = df_arrivals.groupby(group_col)\n",
    "\n",
    "    for idx, row in portcalls_df.iterrows():\n",
    "        ship_key = str(row[group_col])\n",
    "        call_time = row[\"TIMESTAMP_UTC\"]\n",
    "\n",
    "        if ship_key in arrivals_grouped.groups:\n",
    "            vessel_arrivals = arrivals_grouped.get_group(ship_key)\n",
    "            future_calls = vessel_arrivals[vessel_arrivals[\"TIMESTAMP_UTC\"] > call_time]\n",
    "            if not future_calls.empty:\n",
    "                next_arrival = future_calls.sort_values(\"TIMESTAMP_UTC\").iloc[0]\n",
    "                portcalls_df.loc[idx, \"NEXT_PORT_ID\"] = next_arrival.get(\"PORT_ID\", pd.NA)\n",
    "                portcalls_df.loc[idx, \"NEXT_PORT\"] = next_arrival.get(\"PORT_NAME\", pd.NA)\n",
    "                portcalls_df.loc[idx, \"NEXT_PORT_COUNTRY\"] = next_arrival.get(\"PORT_COUNTRY_CODE\", pd.NA)\n",
    "                portcalls_df.loc[idx, \"NEXT_PORT_TIMESTAMP\"] = pd.to_datetime(next_arrival[\"TIMESTAMP_UTC\"]).tz_localize(None)\n",
    "\n",
    "    return portcalls_df\n",
    "\n",
    "def enrich_portcalls_with_last_and_next(portcalls_df, fleet_df, region):\n",
    "    \"\"\"\n",
    "    Runs both last departure and next arrival enrichment for the selected region.\n",
    "    \"\"\"\n",
    "    df = enrich_portcalls_with_last_departure_region(portcalls_df, fleet_df, region)\n",
    "    df = enrich_portcalls_with_next_nonregion_arrival(df, fleet_df, region)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7320cebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# Section 8: Fetch TEU/CAPACITY from MT GraphQL API\n",
    "# ================================================\n",
    "def fetch_teu_by_imo_list(imo_list, batch_size=500):\n",
    "    \"\"\"\n",
    "    Fetch teuCapacity for a list of IMOs using the Kpler GraphQL API.\n",
    "    Returns a DataFrame with columns: IMO, TEU_CAPACITY\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for i in range(0, len(imo_list), batch_size):\n",
    "        batch = imo_list[i:i+batch_size]\n",
    "        query = f\"\"\"\n",
    "        query Vessels {{\n",
    "            vessels(\n",
    "                first: {len(batch)}\n",
    "                where: {{\n",
    "                    filters: [\n",
    "                        {{\n",
    "                            field: \"identifier.imo\"\n",
    "                            op: IN\n",
    "                            values: [{','.join(f'\"{imo}\"' for imo in batch)}]\n",
    "                        }}\n",
    "                    ]\n",
    "                    operator: OR\n",
    "                }}\n",
    "            ) {{\n",
    "                nodes {{\n",
    "                    identifier {{ imo }}\n",
    "                    particulars {{\n",
    "                        capacity {{ teuCapacity }}\n",
    "                    }}\n",
    "                }}\n",
    "            }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Basic {GRAPHQL_API_KEY}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "        response = requests.post(GRAPHQL_URL, json={\"query\": query}, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error {response.status_code}: {response.text}\")\n",
    "            continue\n",
    "\n",
    "        data = response.json()\n",
    "        vessels = data.get(\"data\", {}).get(\"vessels\", {}).get(\"nodes\", [])\n",
    "        for vessel in vessels:\n",
    "            imo = vessel.get(\"identifier\", {}).get(\"imo\")\n",
    "            teu = None\n",
    "            particulars = vessel.get(\"particulars\", {})\n",
    "            if particulars and particulars.get(\"capacity\"):\n",
    "                teu = particulars[\"capacity\"].get(\"teuCapacity\")\n",
    "            results.append({\"IMO\": imo, \"TEU/CAPACITY\": teu})\n",
    "\n",
    "    import pandas as pd\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247964ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# Section 9a: Aggregation of Total Arrivals / Vessel Count to US Ports / TEU/capacity Totals\n",
    "# ================================================\n",
    "def aggregate_total_port_arrivals(df_enriched, output_filename=None):\n",
    "    \"\"\"\n",
    "    Aggregates total arrivals, unique vessel count, and TEU/capacity totals by PORT_ID, PORT_NAME, and MONTH_YEAR.\n",
    "    \"\"\"\n",
    "    # Ensure TIMESTAMP_UTC is datetime\n",
    "    df_enriched[\"TIMESTAMP_UTC\"] = pd.to_datetime(df_enriched[\"TIMESTAMP_UTC\"], errors='coerce', dayfirst=True)\n",
    "    df_enriched[\"MONTH_YEAR\"] = df_enriched[\"TIMESTAMP_UTC\"].dt.strftime('%Y-%m')\n",
    "\n",
    "    agg = df_enriched.groupby(['PORT_ID', 'PORT_NAME', 'MONTH_YEAR']).agg(\n",
    "        total_arrivals=('SHIP_ID', 'count'),\n",
    "        vessel_count=('IMO', 'nunique'),\n",
    "        teu_capacity_total=('TEU/CAPACITY', lambda x: pd.to_numeric(x, errors='coerce').sum())\n",
    "    ).reset_index()\n",
    "\n",
    "    agg['teu_capacity_total'] = agg['teu_capacity_total'].astype('Int64')\n",
    "\n",
    "    if output_filename:\n",
    "        agg.to_csv(output_filename, index=False)\n",
    "        print(f\"Aggregation saved to {output_filename}\")\n",
    "\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d0095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# Section 10: Main Function\n",
    "# ================================================\n",
    "def main():\n",
    "    print(\"MarineTraffic Port Calls + Fleet Management Tool\")\n",
    "    print(\"Aggregation from existing enriched file.\\n\")\n",
    "\n",
    "    try:\n",
    "        # Step 1: User input for Port Calls (any region)\n",
    "        region, from_date, to_date, market_list, movetype = get_user_portcall_input()\n",
    "        region = region.strip().upper()  # Normalize region value\n",
    "\n",
    "        # Step 2: Fetch Port Call Data (batched if needed)\n",
    "        print(\"\\n=== Step 2: Fetch Port Call Data ===\")\n",
    "        data = fetch_portcall_data(region, from_date, to_date, market_list, movetype)\n",
    "        if not data:\n",
    "            print(\"No port call data returned.\")\n",
    "            return\n",
    "\n",
    "        df_portcalls = normalize_portcall_data(data)\n",
    "        print(f\"Fetched {len(df_portcalls)} port call records.\")\n",
    "        portcalls_csv = save_df_to_csv(df_portcalls)\n",
    "        print(f\"✅ Port calls saved to {portcalls_csv}\")\n",
    "\n",
    "        # Step 3: Extract unique IMOs for adding to fleet\n",
    "        imo_list = df_portcalls['IMO'].dropna().astype(str).unique().tolist()\n",
    "\n",
    "        # Step 4: Clear Fleet (TEMPORARILY DISABLED)\n",
    "        print(\"\\n=== Step 4: Clear Fleet ===\")\n",
    "        cleared = clear_fleet(CLEARFLEET_API_KEY, FLEET_ID)\n",
    "        if not cleared:\n",
    "            print(\"Fleet clearing failed. Aborting.\")\n",
    "            return\n",
    "\n",
    "        # Step 5: Add vessels to Fleet (TEMPORARILY DISABLED)\n",
    "        add_vessels_to_fleet(SETFLEET_API_KEY, FLEET_ID, imo_list)\n",
    "\n",
    "        # Step 6: Fetch Fleet Port Calls (batched if needed) and enrich\n",
    "        fleet_filename = save_fleet_portcalls(FLEET_PORTCALL_API_KEY, from_date, to_date, region=region)\n",
    "        if not fleet_filename:\n",
    "            print(\"No fleet data to enrich port calls.\")\n",
    "            return\n",
    "        df_fleet_portcalls = pd.read_csv(fleet_filename)\n",
    "\n",
    "        # Enrich with last non-region departure and next non-region arrival\n",
    "        df_enriched = enrich_portcalls_with_last_and_next(df_portcalls, df_fleet_portcalls, region)\n",
    "        print(f\"Enriched {len(df_enriched)} port call records.\")\n",
    "\n",
    "        # Step 7: Fetch TEU/CAPACITY using GraphQL and merge\n",
    "        print(\"\\n=== Step 7: Fetching TEU/CAPACITY for vessels via GraphQL API ===\")\n",
    "        imo_list = df_enriched['IMO'].dropna().astype(str).unique().tolist()\n",
    "        df_teu = fetch_teu_by_imo_list(imo_list)\n",
    "\n",
    "        df_enriched['IMO'] = df_enriched['IMO'].astype(str)\n",
    "        df_teu['IMO'] = df_teu['IMO'].astype(str)\n",
    "        df_enriched = df_enriched.merge(df_teu, on=\"IMO\", how=\"left\")\n",
    "\n",
    "        cols = df_enriched.columns.tolist()\n",
    "        if \"TEU/CAPACITY\" in cols and \"SHIPNAME\" in cols:\n",
    "            cols.remove(\"TEU/CAPACITY\")\n",
    "            idx = cols.index(\"SHIPNAME\") + 1\n",
    "            cols.insert(idx, \"TEU/CAPACITY\")\n",
    "            df_enriched = df_enriched[cols]\n",
    "\n",
    "        # Arrange columns in desired order\n",
    "        desired_cols = [\n",
    "            \"IMO\", \"SHIP_ID\", \"SHIPNAME\", \"TEU/CAPACITY\",\n",
    "            \"LAST_PORT_ID\", \"LAST_PORT\", \"LAST_PORT_COUNTRY_CODE\", \"LAST_PORT_TIMESTAMP\",\n",
    "            \"PORT_ID\", \"PORT_NAME\", \"LOAD_STATUS\", \"TIMESTAMP_UTC\",\n",
    "            \"NEXT_PORT_ID\", \"NEXT_PORT\", \"NEXT_PORT_COUNTRY\", \"NEXT_PORT_TIMESTAMP\"\n",
    "        ]\n",
    "        final_cols = [col for col in desired_cols if col in df_enriched.columns]\n",
    "        df_enriched = df_enriched[final_cols]\n",
    "\n",
    "        # Format all timestamp columns before saving\n",
    "        for col in [\"TIMESTAMP_UTC\", \"LAST_PORT_TIMESTAMP\", \"NEXT_PORT_TIMESTAMP\"]:\n",
    "            if col in df_enriched.columns:\n",
    "                df_enriched[col] = pd.to_datetime(df_enriched[col], errors='coerce').dt.strftime('%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "        # Now save to CSV\n",
    "        enriched_filename = f\"{region.lower()}_portcalls_enriched_{datetime.utcnow().strftime('%Y%m%d_%H%M')}.csv\"\n",
    "        df_enriched.to_csv(enriched_filename, index=False)\n",
    "        print(f\"✅ Enriched port calls saved to {enriched_filename}\")   \n",
    "\n",
    "        agg_arrivals = aggregate_total_port_arrivals(df_enriched, output_filename=f'{region.lower()}_port_aggregation.csv')\n",
    "        print(f\"✅ Aggregation complete. Output: {region.lower()}_port_aggregation.csv\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nUser exited program.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Unexpected error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
